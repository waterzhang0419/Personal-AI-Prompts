# 使用老顽童周伯通的左右互搏之术，让 AI 自己校验答案

有时候，AI 不知道是出于鼓励还是产生了幻觉，会倾向于给出那种看起来很好的答案。

比如，我让 AI 评估雅思作文，有时候它会给出 7.5 甚至 8 分的评分。要知道，雅思满分 9 分，要提升 1 分可是要付出不小的努力的。

我看到高分评价当然很开心，但还好我每天早晨起来都称了空腹体重，因此，我知道自己几斤几两。我磕磕巴巴写出来的蹩脚作文是铁定达不到这样的分数的。

对于这种情况，我一般是会反问：“你确定这篇文章能达到这么高分数的水平吗？请解释理由。”

这种方式通常可以对模型的输出结果做一个校验。

最近看到吴恩达教授的在线课程，Building Systems with the ChatGPT API. 课程里面有一个章节是关于评估 AI 输出的结果的。也就是通过 prompt，让 AI 结合上下文，对上一步的输出进行评估，检查输出是否客观，是否准确地完成了任务要求。

强烈推荐《Building Systems with the ChatGPT API》这个课程，可以学习到写 prompt 的很多技巧，比如 CoT、提示词链、结果评估等。这些真正厉害的人总是可以把一个东西讲得深入浅出，深者不觉其浅，浅者不觉其深。

虽然具体的 Python 代码是看不懂，但不妨碍我们去理解思路。

在此基础上，受《Enhancing Zero-Shot Chain-of-Thought Reasoning in Large Language Models through Logic》这篇论文的启发，我写了一个让 AI 左右互博的 prompt 来验证 AI 的输出结果。

这篇论文的核心思想是对 CoT（Chain-of-Thought）的每一步推理结果进行二次校验，校验完对这一步的推理过程进行修正，然后重新让 AI 按照新的步骤推理。

在进行二次校验的时候，思路是让 AI 自己针对指定的步骤，从两个不同的角度进行评审，然后依次分析，最后给出它认为更可信的角度。

我想，这正好可以用来做通用的回复评估。让 AI 从两个相反的角度来评估它自己的回复，很好玩，就像老顽童周伯通的左右互搏之术。

思路是让 AI 回忆上下文，思考目标和相关的理论/原则/标准，然后从相反的角度分别进行分析和评审。

[Prompt 评估助手](../prompt_eval/eval.md)

默认是针对整个回复结果做评审，也可以稍微修改 prompt，让其针对回复中的某一项进行评审。